{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "import json\n",
    "import copy\n",
    "from itertools import product\n",
    "from collections import defaultdict, OrderedDict\n",
    "# 3rd Party\n",
    "import torch\n",
    "from transformers import  BertTokenizerFast\n",
    "# Local\n",
    "from gatbert.data import parse_graph_tsv, Sample\n",
    "from gatbert.graph_sample import GraphSample, Edge\n",
    "from gatbert.constants import NodeType, Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(parse_graph_tsv('scrap.tsv'))\n",
    "\n",
    "target = [\"Pakistan\", \"government\"]\n",
    "context = \"We need to stop supporting governmentalists who harbor terrorists .\".split()\n",
    "kb =[\"/c/en/governmentalists\", \"/c/en/pakistan\"]\n",
    "\n",
    "sample = GraphSample(\n",
    "    stance=Stance.FAVOR,\n",
    "    target=target,\n",
    "    context=context,\n",
    "    kb=kb,\n",
    "    edges=[\n",
    "        Edge(len(target) + 5, len(target) + len(context) + 0, 42), # govermenalists in context to /c/en/governmentalists\n",
    "    ]\n",
    ")\n",
    "clean_kb_sample = [uri.split('/')[3] for uri in sample.kb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer(text=sample.target, text_pair=sample.context, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "tokenized_kb = tokenizer(text=clean_kb_sample, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  4501,  2231,   102,  2057,  2342,  2000,  2644,  4637, 10605,\n",
       "          5130,  2040,  6496, 15554,  1012,   102]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'pakistan',\n",
       " 'government',\n",
       " '[SEP]',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'supporting',\n",
       " 'governmental',\n",
       " '##ists',\n",
       " 'who',\n",
       " 'harbor',\n",
       " 'terrorists',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0,  8],\n",
       "         [ 0, 10],\n",
       "         [ 0,  0],\n",
       "         [ 0,  2],\n",
       "         [ 0,  4],\n",
       "         [ 0,  2],\n",
       "         [ 0,  4],\n",
       "         [ 0, 10],\n",
       "         [ 0, 12],\n",
       "         [12, 16],\n",
       "         [ 0,  3],\n",
       "         [ 0,  6],\n",
       "         [ 0, 10],\n",
       "         [ 0,  1],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'governmental', '##ists', 'pakistan', '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_kb['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0, 12],\n",
       "         [12, 16],\n",
       "         [ 0,  8],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_kb['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (node_index, subword_index)\n",
    "pool_inds = []\n",
    "expand_list = defaultdict(list)\n",
    "\n",
    "pool_inds = OrderedDict()\n",
    "\n",
    "new_nodes_index = -1\n",
    "orig_nodes_index = -1\n",
    "\n",
    "# For token subwords, we will split a token's nodes into subwords\n",
    "token_offset_mapping = tokenized_text['offset_mapping'].squeeze()\n",
    "# Handle splitting of token nodes into subword nodes\n",
    "for (subword_index, (start, end)) in enumerate(token_offset_mapping):\n",
    "    new_nodes_index += 1\n",
    "    pool_inds[new_nodes_index] = []\n",
    "\n",
    "    if start != end: # Real character, not a special character\n",
    "        if start == 0: # Start of a token\n",
    "            orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds[new_nodes_index].append(subword_index)\n",
    "\n",
    "# For KB subwords, we plan to pool each into one combined node\n",
    "# Get this working next\n",
    "kb_offset_mapping = tokenized_kb['offset_mapping'].squeeze()\n",
    "for (subword_index, (start, end)) in enumerate(kb_offset_mapping, start=subword_index + 1):\n",
    "    if start == end:\n",
    "        # Special character; skip over\n",
    "        new_nodes_index += 1\n",
    "        pool_inds[new_nodes_index] = []\n",
    "    elif start == 0:\n",
    "        new_nodes_index += 1\n",
    "        pool_inds[new_nodes_index] = []\n",
    "        orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds[new_nodes_index].append(subword_index)\n",
    "num_new_nodes = new_nodes_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ids = torch.concatenate([tokenized_text['input_ids'], tokenized_kb['input_ids']], dim=-1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_indices = []\n",
    "mask_values = []\n",
    "for (new_node_ind, subword_inds) in pool_inds.items():\n",
    "    mask_indices.extend((new_node_ind, subword_ind) for subword_ind in subword_inds)\n",
    "    v = 1 / len(subword_inds)\n",
    "    mask_values.extend(v for _ in subword_inds)\n",
    "\n",
    "mask_indices = torch.tensor(mask_indices).transpose(1, 0)\n",
    "mask_values = torch.tensor(mask_values)\n",
    "node_mask = torch.sparse_coo_tensor(\n",
    "    indices=mask_indices,\n",
    "    values=mask_values,\n",
    "    size=(num_new_nodes, concat_ids.shape[-1]),\n",
    "    is_coalesced=True,\n",
    "    dtype=torch.float,\n",
    "    device=concat_ids.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 17, 18, 19],\n",
       "                       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 18, 19, 20]]),\n",
       "       values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000]),\n",
       "       size=(20, 21), nnz=21, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Edge(head_node_index=7, tail_node_index=12, relation_id=42)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [1],\n",
       "             1: [2],\n",
       "             2: [4],\n",
       "             3: [5],\n",
       "             4: [6],\n",
       "             5: [7],\n",
       "             6: [8],\n",
       "             7: [9, 10],\n",
       "             8: [11],\n",
       "             9: [12],\n",
       "             10: [13],\n",
       "             11: [14],\n",
       "             12: [17],\n",
       "             13: [18]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edges = []\n",
    "for edge in sample.edges:\n",
    "    if edge.head_node_index not in expand_list:\n",
    "        print(f\"Warning: found no expansions for node {edge.head_node_index}\")\n",
    "        continue\n",
    "    head_expand_list = expand_list[edge.head_node_index]\n",
    "    if edge.tail_node_index not in expand_list:\n",
    "        print(f\"Warning: found no expansions for node {edge.tail_node_index}\")\n",
    "        continue\n",
    "    tail_expand_list = expand_list[edge.tail_node_index]\n",
    "    new_edges.extend((head, tail, edge.relation_id) for (head, tail) in product(head_expand_list, tail_expand_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 17, 42), (10, 17, 42)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'pakistan',\n",
       " 'government',\n",
       " '[SEP]',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'supporting',\n",
       " 'governmental',\n",
       " '##ists',\n",
       " 'who',\n",
       " 'harbor',\n",
       " 'terrorists',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[CLS]',\n",
       " 'governmental',\n",
       " '##ists',\n",
       " 'pakistan',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(concat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 21])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
