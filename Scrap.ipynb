{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL\n",
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# 3rd Party\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(0)\n",
    "from transformers import BertModel, AutoModel, BertTokenizerFast, AutoTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers.pre_tokenizers import BertPreTokenizer, PreTokenizer\n",
    "# Local\n",
    "from gatbert.constants import DEFAULT_MODEL\n",
    "from gatbert.datasets import MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def make_encoder(tokenizer: PreTrainedTokenizerFast, pretokenizer: Optional[PreTokenizer] = None, make_fake_edges=True):\n",
    "    relation_types = [\n",
    "        0, # Token-to-Token\n",
    "        1, # Token-to-CN\n",
    "        2, # CN-to-Token\n",
    "        3, # CN-to-CN\n",
    "    ]\n",
    "\n",
    "    gen = torch.Generator().manual_seed(0)\n",
    "\n",
    "    def encode_sample(sample: Dict[str, Any]):\n",
    "        context: str = sample['context']\n",
    "        target: str = sample['target']\n",
    "        stance: int = sample['stance']\n",
    "\n",
    "        if pretokenizer:\n",
    "            pre_context = [pair[0] for pair in pretokenizer.pre_tokenize_str(context)]\n",
    "            pre_target = [pair[0] for pair in pretokenizer.pre_tokenize_str(target)]\n",
    "            result = tokenizer(text=pre_target, text_pair=pre_context, is_split_into_words=True, return_tensors='pt')\n",
    "        else:\n",
    "            result = tokenizer(text=target, text_pair=context)\n",
    "\n",
    "        result = {k: torch.squeeze(v) for (k, v) in result.items()}\n",
    "        n_text_nodes = len(result['input_ids'])\n",
    "\n",
    "\n",
    "        edge_ids = []\n",
    "        for head in range(n_text_nodes):\n",
    "            edge_ids.append( (head, tail, 0) )\n",
    "            for tail in range(head + 1, n_text_nodes):\n",
    "                edge_ids.append( (head, tail, 0) )\n",
    "                edge_ids.append( (tail, head, 0) )\n",
    "        total_nodes = n_text_nodes\n",
    "        if make_fake_edges:\n",
    "            n_fake_cn_nodes = torch.randint(3, 5, size=(), generator=gen)\n",
    "            for (token_id, cn_id) in zip(*torch.where(torch.randn(n_text_nodes, n_fake_cn_nodes, generator=gen) < .1)):\n",
    "                edge_ids.append( (token_id, cn_id + n_text_nodes, 1) )\n",
    "            for (token_id, cn_id) in zip(*torch.where(torch.randn(n_text_nodes, n_fake_cn_nodes, generator=gen) < .1)):\n",
    "                edge_ids.append( (cn_id + n_text_nodes, token_id, 2) )\n",
    "            for (cn_id, cn_id_b) in zip(*torch.where(torch.randn(n_fake_cn_nodes, n_fake_cn_nodes, generator=gen) < .1)):\n",
    "                edge_ids.append( (cn_id + n_text_nodes, cn_id_b + n_text_nodes, 3) )\n",
    "            total_nodes += n_fake_cn_nodes\n",
    "        edge_ids.sort()\n",
    "        sparse_ids = torch.tensor(edge_ids).transpose(1, 0)\n",
    "        result['edges'] = sparse_ids\n",
    "        result['nodes'] = total_nodes\n",
    "        result['stance'] = torch.tensor(stance)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return encode_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=True)\n",
    "encoder = make_encoder(tokenizer, BertPreTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_samples = [\n",
    "    { \n",
    "    \"context\": \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\",\n",
    "    \"target\": \"Independence from Britain\",\n",
    "    \"stance\": 2\n",
    "    },\n",
    "    { \n",
    "    \"context\": \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\",\n",
    "    \"target\": \"Social Security\",\n",
    "    \"stance\": 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(tokenizer):\n",
    "    def collate_fn(samples: Dict[str, Any]):\n",
    "        batched = {}\n",
    "        token_padding = tokenizer.pad_token_id\n",
    "        type_padding = tokenizer.pad_token_type_id\n",
    "        batched['input_ids'] = torch.nn.utils.rnn.pad_sequence([s['input_ids'] for s in samples], batch_first=True, padding_value=token_padding)\n",
    "        batched['token_type_ids'] = torch.nn.utils.rnn.pad_sequence([s['token_type_ids'] for s in samples], batch_first=True, padding_value=type_padding)\n",
    "        batched['attention_mask'] = batched['input_ids'] != token_padding\n",
    "        batched['stance'] = torch.stack([s['stance'] for s in samples], dim=0)\n",
    "\n",
    "        batch_edges = []\n",
    "        for (i, sample_edges) in enumerate(map(lambda s: s['edges'], samples)):\n",
    "            batch_edges.append(torch.concatenate([\n",
    "                torch.full(size=(1, sample_edges.shape[1]), fill_value=i),\n",
    "                sample_edges\n",
    "            ]))\n",
    "        batched['edges'] = torch.concatenate(batch_edges, dim=-1)\n",
    "        return batched\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MapDataset([encoder(s) for s in fake_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(ds, batch_size=2, shuffle=False, collate_fn=make_collate_fn(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7824,  1121,  2855,   102,  1284,  2080,  1292,  3062,  1116,\n",
      "          1106,  1129,  2191,   118, 10238,   117,  1115,  1155,  1441,  1132,\n",
      "          1687,  4463,   117,  1115,  1152,  1132, 22868,  1118,  1147,   140,\n",
      "         26284,  1114,  2218,  8362, 10584,  7076,  2165,  5399,   117,  1115,\n",
      "          1621,  1292,  1132,  2583,   117,  8146,  1105,  1103,  9542,  1104,\n",
      "         25410,   119,   102],\n",
      "        [  101,  3563,  4354,   102,  3396,  2794,  1105,  1978,  1201,  2403,\n",
      "          1412, 15920,  1814,  5275,  1113,  1142, 10995,   117,   170,  1207,\n",
      "          3790,   117, 10187,  1107,  8146,   117,  1105,  3256,  1106,  1103,\n",
      "         21133,  1115,  1155,  1441,  1132,  1687,  4463,   119,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False]]), 'stance': tensor([2, 0]), 'edges': tensor([[ 0,  0,  0,  ...,  1,  1,  1],\n",
      "        [ 0,  0,  0,  ..., 42, 42, 42],\n",
      "        [ 1,  2,  3,  ..., 38, 39, 40],\n",
      "        [ 0,  0,  0,  ...,  2,  3,  3]])}\n"
     ]
    }
   ],
   "source": [
    "for d in loader:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4591])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['edges'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 123\n",
    "attention_units = 53\n",
    "out_features = 264\n",
    "n_heads = 6\n",
    "n_relations = 7\n",
    "n_bases = 3\n",
    "max_nodes = 10\n",
    "batch_size = 5\n",
    "gen = torch.Generator().manual_seed(1)\n",
    "random_features = 5 * (torch.randn(batch_size, max_nodes, in_features, generator=gen) - .5)\n",
    "random_features.shape\n",
    "random_adj = torch.randint(0, 2, size=[batch_size, max_nodes, max_nodes, n_relations], generator=gen).to_sparse()\n",
    "random_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
