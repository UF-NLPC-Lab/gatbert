{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "# 3rd Party\n",
    "import torch\n",
    "from transformers import  BertTokenizerFast\n",
    "# Local\n",
    "from gatbert.data import parse_graph_tsv, Sample\n",
    "from gatbert.graph_sample import GraphSample, Edge\n",
    "from gatbert.constants import NodeType, Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(parse_graph_tsv('scrap.tsv'))\n",
    "\n",
    "sample = GraphSample(\n",
    "    stance=Stance.FAVOR,\n",
    "    target=[\"Pakistan\", \"government\"],\n",
    "    context=\"We need to stop supporting governmentalists who harbor terrorists .\".split(),\n",
    "    kb=[\"/c/en/governmentalists\", \"/c/en/pakistan\"],\n",
    "    edges=[]\n",
    ")\n",
    "clean_kb_sample = [uri.split('/')[3] for uri in sample.kb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer(text=sample.target, text_pair=sample.context, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "tokenized_kb = tokenizer(text=clean_kb_sample, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  4501,  2231,   102,  2057,  2342,  2000,  2644,  4637, 10605,\n",
       "          5130,  2040,  6496, 15554,  1012,   102]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'pakistan',\n",
       " 'government',\n",
       " '[SEP]',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'supporting',\n",
       " 'governmental',\n",
       " '##ists',\n",
       " 'who',\n",
       " 'harbor',\n",
       " 'terrorists',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0,  8],\n",
       "         [ 0, 10],\n",
       "         [ 0,  0],\n",
       "         [ 0,  2],\n",
       "         [ 0,  4],\n",
       "         [ 0,  2],\n",
       "         [ 0,  4],\n",
       "         [ 0, 10],\n",
       "         [ 0, 12],\n",
       "         [12, 16],\n",
       "         [ 0,  3],\n",
       "         [ 0,  6],\n",
       "         [ 0, 10],\n",
       "         [ 0,  1],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'governmental', '##ists', 'pakistan', '[SEP]']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_kb['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0, 12],\n",
       "         [12, 16],\n",
       "         [ 0,  8],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_kb['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (node_index, subword_index)\n",
    "pool_inds = []\n",
    "expand_list = defaultdict(list)\n",
    "\n",
    "pool_inds = OrderedDict()\n",
    "\n",
    "new_nodes_index = -1\n",
    "orig_nodes_index = -1\n",
    "\n",
    "# For token subwords, we will split a token's nodes into subwords\n",
    "token_offset_mapping = tokenized_text['offset_mapping'].squeeze()\n",
    "# Handle splitting of token nodes into subword nodes\n",
    "for (subword_index, (start, end)) in enumerate(token_offset_mapping):\n",
    "    new_nodes_index += 1\n",
    "    pool_inds[new_nodes_index] = []\n",
    "\n",
    "    if start != end: # Real character, not a special character\n",
    "        if start == 0: # Start of a token\n",
    "            orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds[new_nodes_index].append(subword_index)\n",
    "\n",
    "# For KB subwords, we plan to pool each into one combined node\n",
    "# Get this working next\n",
    "kb_offset_mapping = tokenized_kb['offset_mapping'].squeeze()\n",
    "for (subword_index, (start, end)) in enumerate(kb_offset_mapping, start=subword_index + 1):\n",
    "    if start == end:\n",
    "        # Special character; skip over\n",
    "        new_nodes_index += 1\n",
    "        pool_inds[new_nodes_index] = []\n",
    "    elif start == 0:\n",
    "        new_nodes_index += 1\n",
    "        pool_inds[new_nodes_index] = []\n",
    "        orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds[new_nodes_index].append(subword_index)\n",
    "num_new_nodes = new_nodes_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ids = torch.concatenate([tokenized_text['input_ids'], tokenized_kb['input_ids']], dim=-1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_indices = []\n",
    "mask_values = []\n",
    "for (new_node_ind, subword_inds) in pool_inds.items():\n",
    "    mask_indices.extend((new_node_ind, subword_ind) for subword_ind in subword_inds)\n",
    "    v = 1 / len(subword_inds)\n",
    "    mask_values.extend(v for _ in subword_inds)\n",
    "\n",
    "mask_indices = torch.tensor(mask_indices).transpose(1, 0)\n",
    "mask_values = torch.tensor(mask_values)\n",
    "node_mask = torch.sparse_coo_tensor(\n",
    "    indices=mask_indices,\n",
    "    values=mask_values,\n",
    "    size=(num_new_nodes, concat_ids.shape[-1]),\n",
    "    is_coalesced=True,\n",
    "    dtype=torch.float,\n",
    "    device=concat_ids.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 17, 18, 19],\n",
       "                       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 18, 19, 20]]),\n",
       "       values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "                      1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000]),\n",
       "       size=(20, 21), nnz=21, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
