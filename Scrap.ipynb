{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethanlmines/blue_dir/conda_envs/gatbert/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# STL\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# 3rd Party\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "# Local\n",
    "from gatbert.data import parse_graph_tsv\n",
    "from gatbert.graph_sample import GraphSample, Edge\n",
    "from gatbert.constants import Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 45986, 46021, 9, 796, 1332, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (5, 12), (13, 15), (16, 24), (25, 30), (0, 0)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>', 'Ġabbre', 'viation', 'Ġof', 'ĠEuropean', 'ĠUnion', '</s>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rob_tokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-MNLI\", use_fast=True, add_prefix_space=True)\n",
    "rob_encoding = rob_tokenizer(['abbreviation of European Union'], is_split_into_words=True, return_offsets_mapping=True)\n",
    "print(rob_encoding)\n",
    "rob_tokenizer.convert_ids_to_tokens(rob_encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 22498, 1997, 2647, 2586, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 12), (13, 15), (16, 24), (25, 30), (0, 0)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abbreviation', 'of', 'european', 'union', '[SEP]']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, use_fast=True, add_prefix_space=True)\n",
    "encoding = tokenizer(['abbreviation of european union'], is_split_into_words=True, return_offsets_mapping=True)\n",
    "print(encoding)\n",
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"Pakistan\", \"government\"]\n",
    "context = \"We need to stop supporting governmentalists who harbor terrorists .\".split()\n",
    "kb =[\"/c/en/governmentalists\", \"/c/en/pakistan\"]\n",
    "sample = GraphSample(\n",
    "    stance=Stance.FAVOR,\n",
    "    target=target,\n",
    "    context=context,\n",
    "    kb=kb,\n",
    "    edges=[\n",
    "        Edge(len(target) + 5, len(target) + len(context) + 0, 42), # govermenalists in context to /c/en/governmentalists\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_gen = parse_graph_tsv('scrap2.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = next(graph_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gatbert.constants import MAX_KB_NODES, TOKEN_TO_TOKEN_RELATION_ID\n",
    "import logging\n",
    "from itertools import product\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "# FIXME: This is assuming all the KB tokens are conceptnet URIs\n",
    "clean_kb = [uri.split('/')[3] for uri in self.kb]\n",
    "clean_kb = [uri.replace(\"_\", ' ') for uri in clean_kb]\n",
    "\n",
    "tokenized_text = tokenizer(text=self.target,\n",
    "                           text_pair=self.context,\n",
    "                           is_split_into_words=True,\n",
    "                           return_offsets_mapping=True,\n",
    "                           return_tensors='pt',\n",
    "                           truncation='longest_first')\n",
    "tokenized_kb = tokenizer(text=clean_kb,\n",
    "                         is_split_into_words=True,\n",
    "                         return_offsets_mapping=True,\n",
    "                         return_tensors='pt')\n",
    "device = tokenized_text['input_ids'].device\n",
    "\n",
    "relevant_keys = ['input_ids', 'offset_mapping']\n",
    "tokenized_text = {k:tokenized_text[k] for k in relevant_keys}\n",
    "tokenized_kb = {k:tokenized_kb[k] for k in relevant_keys}\n",
    "# Assumes the tokens with 0-length offsets are special tokens\n",
    "real_toks = tokenized_kb['offset_mapping'][0, :, 1] != 0\n",
    "for k in relevant_keys:\n",
    "    tokenized_kb[k] = tokenized_kb[k][:, real_toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(self.kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Discarded 207/335 of external nodes\n"
     ]
    }
   ],
   "source": [
    "# old_node_index -> [new_node_indices]\n",
    "expand_list = defaultdict(list)\n",
    "# new_node_index -> [subword_indices]\n",
    "pool_inds = OrderedDict()\n",
    "\n",
    "new_nodes_index = -1\n",
    "orig_nodes_index = -1\n",
    "\n",
    "# For token subwords, we will split a token's nodes into subwords\n",
    "token_offset_mapping = tokenized_text['offset_mapping'].squeeze()\n",
    "# Handle splitting of token nodes into subword nodes\n",
    "for (subword_index, (start, end)) in enumerate(token_offset_mapping):\n",
    "    new_nodes_index += 1\n",
    "    pool_inds[new_nodes_index] = []\n",
    "\n",
    "    if start != end: # Real character, not a special character\n",
    "        if start == 0: # Start of a token\n",
    "            orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds[new_nodes_index].append(subword_index)\n",
    "\n",
    "\n",
    "# Need to fast-forward past the token nodes to the external ones\n",
    "# Some of the token nodes may have been truncated by the tokenizer\n",
    "orig_nodes_index = len(self.target) + len(self.context) - 1\n",
    "\n",
    "# For KB subwords, we plan to pool each into one combined node\n",
    "kb_offset_mapping = tokenized_kb['offset_mapping'].squeeze()\n",
    "n_kb_nodes = 0\n",
    "for (subword_index, (start, end)) in enumerate(kb_offset_mapping, start=subword_index + 1):\n",
    "    if start == 0:\n",
    "        assert end != 0, \"Special tokens should have been scrubbed\"\n",
    "        if n_kb_nodes >= MAX_KB_NODES:\n",
    "            logging.warning(\"Discarded %s/%s of external nodes\", len(self.kb) - n_kb_nodes, len(self.kb))\n",
    "            break\n",
    "        n_kb_nodes += 1\n",
    "        new_nodes_index += 1\n",
    "        pool_inds[new_nodes_index] = []\n",
    "        orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds[new_nodes_index].append(subword_index)\n",
    "else:\n",
    "    # Needs to be 1 greater than the last subword we included\n",
    "    subword_index += 1\n",
    "\n",
    "concat_ids = torch.concatenate([tokenized_text['input_ids'], tokenized_kb['input_ids']], dim=-1).squeeze()\n",
    "# The tokenizer already did truncation for tokens, but this is where we do truncation for external nodes\n",
    "concat_ids = concat_ids[..., :subword_index]\n",
    "\n",
    "num_new_nodes = new_nodes_index + 1\n",
    "\n",
    "mask_indices = []\n",
    "mask_values = []\n",
    "for (new_node_ind, subword_inds) in pool_inds.items():\n",
    "    mask_indices.extend((0, new_node_ind, subword_ind) for subword_ind in subword_inds)\n",
    "    v = 1 / len(subword_inds)\n",
    "    mask_values.extend(v for _ in subword_inds)\n",
    "\n",
    "mask_indices = torch.tensor(mask_indices, device=device).transpose(1, 0)\n",
    "mask_values = torch.tensor(mask_values, device=device)\n",
    "node_mask = torch.sparse_coo_tensor(\n",
    "    indices=mask_indices,\n",
    "    values=mask_values,\n",
    "    size=(1, num_new_nodes, concat_ids.shape[-1]),\n",
    "    is_coalesced=True,\n",
    "    dtype=torch.float,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Indices into a sparse array (batch, max_new_nodes, max_new_nodes, relation)\n",
    "# Need a 0 at the beginning for batch\n",
    "new_edges = []\n",
    "# The original token-to-token edges of a standard BERT model\n",
    "num_text_tokens = tokenized_text['input_ids'].shape[-1]\n",
    "new_edges.extend((0, head, tail, TOKEN_TO_TOKEN_RELATION_ID) for (head, tail) in product(range(num_text_tokens), range(num_text_tokens)))\n",
    "new_edges.extend((0, tail, head, TOKEN_TO_TOKEN_RELATION_ID) for (head, tail) in product(range(num_text_tokens), range(num_text_tokens)))\n",
    "\n",
    "# The edges that we read from the file.\n",
    "# Update their head/tail indices to account for subwords and special tokens\n",
    "discarded = 0\n",
    "for edge in self.edges:\n",
    "    if edge.head_node_index not in expand_list or edge.tail_node_index not in expand_list:\n",
    "        discarded += 1\n",
    "        continue\n",
    "    expand_list[edge.tail_node_index]\n",
    "    new_edges.extend((0, head, tail, edge.relation_id) for (head, tail) in product(expand_list[edge.head_node_index], expand_list[edge.tail_node_index]))\n",
    "new_edges.sort()\n",
    "logging.debug(\"Discarded %s/%s edges.\", discarded, len(self.edges))\n",
    "\n",
    "new_edges = torch.tensor(new_edges, device=device).transpose(1, 0)\n",
    "rval = {\n",
    "    \"input_ids\" : concat_ids,\n",
    "    \"node_mask\" : node_mask,\n",
    "    \"edge_indices\": new_edges\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([307]), torch.Size([1, 197, 307]), torch.Size([4, 9754]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rval['input_ids'].shape, rval['node_mask'].shape, rval['edge_indices'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 69])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
