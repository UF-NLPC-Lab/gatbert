{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethanlmines/blue_dir/conda_envs/gatbert/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# STL\n",
    "import os\n",
    "import json\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# 3rd Party\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from transformers import  BertTokenizerFast\n",
    "# Local\n",
    "from gatbert.data import parse_graph_tsv\n",
    "from gatbert.constants import NodeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(parse_graph_tsv('scrap.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2817 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(text=sample.target, text_pair=sample.context, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "tokenized_kb = tokenizer(text=sample.kb, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_offsets = torch.concatenate([tokenized_text['offset_mapping'].squeeze(), tokenized_text['offset_mapping'].squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# (node_index, subword_index)\n",
    "pool_inds = []\n",
    "\n",
    "expand_list = defaultdict(list)\n",
    "\n",
    "new_nodes_index = 0\n",
    "orig_nodes_index = 0\n",
    "\n",
    "token_offset_mapping = tokenized_text['offset_mapping'].squeeze()\n",
    "\n",
    "# Handle splitting of token nodes into subword nodes\n",
    "for (subword_index, (start, end)) in enumerate(token_offset_mapping):\n",
    "    if end == start:\n",
    "        # Special character; skip over\n",
    "        pool_inds.append((new_nodes_index, subword_index))\n",
    "    else:\n",
    "        if start == 0:\n",
    "            orig_nodes_index += 1\n",
    "        pool_inds.append((new_nodes_index, subword_index))\n",
    "        expand_list[orig_nodes_index].append(subword_index)\n",
    "    new_nodes_index += 1\n",
    "\n",
    "# Get this working next\n",
    "n_subwords = subword_index + 1\n",
    "for (subword_index, (start, end)) in enumerate(tokenized_kb['offset_mapping'].squeeze()):\n",
    "    if end == start:\n",
    "        continue\n",
    "    if start == 0:\n",
    "        pass\n",
    "    pool_inds.append((new_nodes_index, subword_index))\n",
    "    new_nodes_index += 1\n",
    "\n",
    "\n",
    "# Handle pool indices for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_edges = []\n",
    "for edge in sample.edges:\n",
    "    head_inds = expand\n",
    "    tail_inds = []\n",
    "\n",
    "    if edge_cop.head_graph_index == NodeType.TOKEN:\n",
    "        for subword_ind in expand_list[edge.head_node_index]:\n",
    "            edge_cop = deepcopy(edge)\n",
    "\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split2orig = []\n",
    "cur_index = 0\n",
    "for (i, (offset, length)) in enumerate(results['offset_mapping']):\n",
    "    if length == 0:\n",
    "        cur_index += 1\n",
    "    elif offset == 0:\n",
    "        cur_index += 1\n",
    "        split2orig.append(cur_index)\n",
    "    else:\n",
    "        split2orig.append(cur_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Pakistan',\n",
       " 'government',\n",
       " '[SEP]',\n",
       " 'Pakistan',\n",
       " 'government',\n",
       " 'Re',\n",
       " '##quest',\n",
       " 'more',\n",
       " 'help',\n",
       " 'from',\n",
       " 'the',\n",
       " 'world',\n",
       " 'community',\n",
       " '.',\n",
       " 'after',\n",
       " 'facing',\n",
       " 'the',\n",
       " 'flood',\n",
       " 'The',\n",
       " 'largest',\n",
       " 'in',\n",
       " 'history',\n",
       " 'causing',\n",
       " 'heavy',\n",
       " 'damage',\n",
       " 'across',\n",
       " 'the',\n",
       " 'country',\n",
       " '1',\n",
       " ',',\n",
       " '129',\n",
       " 'people',\n",
       " 'have',\n",
       " 'died',\n",
       " ',',\n",
       " '5',\n",
       " ',',\n",
       " '000',\n",
       " 'people',\n",
       " 'have',\n",
       " 'been',\n",
       " 'injured',\n",
       " ',',\n",
       " 'and',\n",
       " '50',\n",
       " 'million',\n",
       " 'people',\n",
       " 'have',\n",
       " 'been',\n",
       " 'evacuated',\n",
       " '.',\n",
       " 'C',\n",
       " '##r',\n",
       " ':',\n",
       " 'T',\n",
       " '##N',\n",
       " '##N',\n",
       " 'World',\n",
       " 'News',\n",
       " 'The',\n",
       " 'following',\n",
       " 'media',\n",
       " 'includes',\n",
       " 'potentially',\n",
       " 'sensitive',\n",
       " 'content',\n",
       " '.',\n",
       " 'Change',\n",
       " 'settings',\n",
       " 'View',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(results['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]',\n",
       "  'higher',\n",
       "  '_',\n",
       "  'number',\n",
       "  '_',\n",
       "  'than',\n",
       "  '_',\n",
       "  '0',\n",
       "  'b',\n",
       "  '##ool',\n",
       "  '##ean',\n",
       "  'ps',\n",
       "  '_',\n",
       "  '1',\n",
       "  'internet',\n",
       "  'b',\n",
       "  '##ool',\n",
       "  '##ean',\n",
       "  '[SEP]'],\n",
       " [(0, 0),\n",
       "  (0, 6),\n",
       "  (6, 7),\n",
       "  (7, 13),\n",
       "  (13, 14),\n",
       "  (14, 18),\n",
       "  (18, 19),\n",
       "  (19, 20),\n",
       "  (0, 1),\n",
       "  (1, 4),\n",
       "  (4, 7),\n",
       "  (0, 2),\n",
       "  (2, 3),\n",
       "  (3, 4),\n",
       "  (0, 8),\n",
       "  (0, 1),\n",
       "  (1, 4),\n",
       "  (4, 7),\n",
       "  (0, 0)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_names = [uri.split('/')[3] for uri in sample.external][-5:]\n",
    "res2 = tokenizer(text=cn_names, is_split_into_words=True, return_offsets_mapping=True)\n",
    "tokenizer.convert_ids_to_tokens(res2['input_ids'] ), res2['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/c/en/pakistan',\n",
       " '/c/en/government',\n",
       " '/c/en/request',\n",
       " '/c/en/more',\n",
       " '/c/en/help',\n",
       " '/c/en/from',\n",
       " '/c/en/the',\n",
       " '/c/en/world',\n",
       " '/c/en/community',\n",
       " '/c/en/after',\n",
       " '/c/en/facing',\n",
       " '/c/en/flood',\n",
       " '/c/en/largest',\n",
       " '/c/en/in',\n",
       " '/c/en/history',\n",
       " '/c/en/causing',\n",
       " '/c/en/heavy',\n",
       " '/c/en/damage',\n",
       " '/c/en/across',\n",
       " '/c/en/country',\n",
       " '/c/en/1',\n",
       " '/c/en/people',\n",
       " '/c/en/have',\n",
       " '/c/en/died',\n",
       " '/c/en/5',\n",
       " '/c/en/000',\n",
       " '/c/en/been',\n",
       " '/c/en/injured',\n",
       " '/c/en/and',\n",
       " '/c/en/50',\n",
       " '/c/en/million',\n",
       " '/c/en/evacuated',\n",
       " '/c/en/cr',\n",
       " '/c/en/tnn',\n",
       " '/c/en/news',\n",
       " '/c/en/following',\n",
       " '/c/en/media',\n",
       " '/c/en/includes',\n",
       " '/c/en/potentially',\n",
       " '/c/en/sensitive',\n",
       " '/c/en/content',\n",
       " '/c/en/change',\n",
       " '/c/en/settings',\n",
       " '/c/en/view',\n",
       " '/c/en/drop_tower/n',\n",
       " '/c/en/vacuum_desiccator/n',\n",
       " '/c/en/unevacuated',\n",
       " '/c/en/vacuum_tube/n',\n",
       " '/c/en/thermionic_valve/n',\n",
       " '/c/en/electron_tube/n',\n",
       " '/c/en/aneroid_barometer/n',\n",
       " '/c/en/evacuable/a',\n",
       " '/c/en/evacuee/n',\n",
       " '/c/en/evacuate/v',\n",
       " '/c/en/5ever',\n",
       " '/c/en/album/n',\n",
       " '/c/en/five',\n",
       " '/c/en/5_cell',\n",
       " '/c/en/arabic_numeral/n',\n",
       " '/c/en/locomotive/n',\n",
       " '/c/en/higher_number_than_4',\n",
       " '/c/en/album',\n",
       " '/c/en/characters',\n",
       " '/c/en/five/n',\n",
       " '/c/en/5s/n',\n",
       " '/c/en/5ns/n',\n",
       " '/c/en/folk_music',\n",
       " '/c/en/number',\n",
       " '/c/en/cat_5',\n",
       " '/c/en/112',\n",
       " '/c/en/999',\n",
       " '/c/en/911',\n",
       " '/c/en/triple_o',\n",
       " '/c/en/triple_o/n',\n",
       " '/c/en/sanitize/v',\n",
       " '/c/en/mosquito_bite/n',\n",
       " '/c/en/amphoterrible/n',\n",
       " '/c/en/top_edge/n',\n",
       " '/c/en/status_epilepticus/n',\n",
       " '/c/en/potential',\n",
       " '/c/en/danger',\n",
       " '/c/en/tasty/a',\n",
       " '/c/en/equipotentially',\n",
       " '/c/en/cutting/a',\n",
       " '/c/en/black_babies/n',\n",
       " '/c/en/in_potentia/r',\n",
       " '/c/en/buffer_state/n',\n",
       " '/c/en/toxic/a',\n",
       " '/c/en/disaster_waiting_to_happen/n',\n",
       " '/c/en/bottom_edge/n',\n",
       " '/c/en/rode_hard_and_put_up_wet/a',\n",
       " '/c/en/50_gon',\n",
       " '/c/en/fifty',\n",
       " '/c/en/automobile/n',\n",
       " '/c/en/fivety',\n",
       " '/c/en/being_involved_in_accident',\n",
       " '/c/en/infirmary/n',\n",
       " '/c/en/fucked_up/a',\n",
       " '/c/en/epimeletic/a',\n",
       " '/c/en/wound',\n",
       " '/c/en/technical_knockout/n',\n",
       " '/c/en/bleed/v',\n",
       " '/c/en/ambulance/n',\n",
       " '/c/en/involved_in_accident',\n",
       " '/c/en/cryoinjured',\n",
       " '/c/en/noninjured',\n",
       " '/c/en/dog_bag/n',\n",
       " '/c/en/hurt/a',\n",
       " '/c/en/stretcher_bearer/n',\n",
       " '/c/en/social_insurance/n',\n",
       " '/c/en/hurt',\n",
       " '/c/en/banged_up/a',\n",
       " '/c/en/in_wars/a',\n",
       " '/c/en/undamaged/a',\n",
       " '/c/en/unscathed/a',\n",
       " '/c/en/sickhouse/n',\n",
       " '/c/en/cryolesioned/a',\n",
       " '/c/en/gammy/a/wikt/en_1',\n",
       " '/c/en/stretcher_case/n',\n",
       " \"/c/en/devil's_antlers/n\",\n",
       " '/c/en/damaged/a',\n",
       " '/c/en/crocked/a/wikt/en_1',\n",
       " '/c/en/damageable/a',\n",
       " '/c/en/wheelchair_user/n',\n",
       " '/c/en/scar',\n",
       " '/c/en/jacked_up/a',\n",
       " '/c/en/spine_board/n',\n",
       " '/c/en/going_into_coma',\n",
       " '/c/en/sore/n/wikt/en_1',\n",
       " '/c/en/uninjured',\n",
       " '/c/en/born',\n",
       " '/c/en/die/v',\n",
       " '/c/en/croak',\n",
       " '/c/en/memorandum/n',\n",
       " '/c/en/cross',\n",
       " '/c/en/too_many_iraqi_children',\n",
       " '/c/en/grandmother_got_very_old',\n",
       " '/c/en/d/a',\n",
       " '/c/en/ob/v/wikt/en_1',\n",
       " '/c/en/widow/n',\n",
       " '/c/en/widower/n',\n",
       " '/c/en/bury_cat',\n",
       " '/c/en/lost_with_all_hands/a',\n",
       " '/c/en/drowned_person',\n",
       " '/c/en/widow',\n",
       " '/c/en/necrology/n',\n",
       " '/c/en/recludes',\n",
       " '/c/en/head',\n",
       " '/c/en/court',\n",
       " '/c/en/precludes',\n",
       " '/c/en/hong_kong/n',\n",
       " '/c/en/excludes',\n",
       " '/c/en/all',\n",
       " '/c/en/reincludes',\n",
       " '/c/en/sky',\n",
       " '/c/en/art',\n",
       " '/c/en/include/v',\n",
       " '/c/en/body',\n",
       " '/c/en/heat',\n",
       " '/c/en/preference',\n",
       " '/c/en/vault/n/wikt/en_1',\n",
       " '/c/en/carriage_return/n',\n",
       " '/c/en/or',\n",
       " '/c/en/wr',\n",
       " '/c/en/sb',\n",
       " '/c/en/continuing_resolution/n',\n",
       " '/c/en/crs/n',\n",
       " '/c/en/sr/n',\n",
       " '/c/en/tr/n',\n",
       " '/c/en/crore',\n",
       " '/c/en/sr',\n",
       " '/c/en/pb',\n",
       " '/c/en/remission/n',\n",
       " '/c/en/dr/n',\n",
       " '/c/en/czech_republic/n',\n",
       " '/c/en/nr',\n",
       " '/c/en/eol',\n",
       " '/c/en/opposite',\n",
       " '/c/en/full_faced/a',\n",
       " '/c/en/adret/n',\n",
       " '/c/en/facingly/r',\n",
       " '/c/en/offside/n',\n",
       " '/c/en/point_presser/n',\n",
       " '/c/en/face/v',\n",
       " '/c/en/understitch/v',\n",
       " '/c/en/counter_rampant/a',\n",
       " '/c/en/eye_to_eye/r',\n",
       " '/c/en/front',\n",
       " '/c/en/topstitch/n',\n",
       " '/c/en/adjacent/a',\n",
       " '/c/en/skyward/a',\n",
       " '/c/en/prolateral/a',\n",
       " '/c/en/facings/n',\n",
       " '/c/en/silly/a',\n",
       " '/c/en/pitching/n',\n",
       " '/c/en/confronté/a',\n",
       " '/c/en/pick_dressing/n',\n",
       " '/c/en/endofacial/a',\n",
       " '/c/en/vis_à_vis',\n",
       " '/c/en/facingly',\n",
       " '/c/en/nearside/n',\n",
       " '/c/en/understitch/n',\n",
       " '/c/en/exofacial/a',\n",
       " '/c/en/kiss',\n",
       " '/c/en/trailing/a',\n",
       " '/c/en/head_to_head/r',\n",
       " '/c/en/facework/n',\n",
       " '/c/en/double_page_spread/n',\n",
       " '/c/en/asbestos',\n",
       " '/c/en/road',\n",
       " '/c/en/bank',\n",
       " '/c/en/wool',\n",
       " '/c/en/done',\n",
       " '/c/en/beest',\n",
       " '/c/en/being',\n",
       " '/c/en/deliver',\n",
       " '/c/en/invention',\n",
       " '/c/en/cotton',\n",
       " '/c/en/squirrel',\n",
       " '/c/en/ruin',\n",
       " '/c/en/charge',\n",
       " '/c/en/money',\n",
       " '/c/en/past_perfect_continuous/n',\n",
       " '/c/en/product',\n",
       " '/c/en/baby',\n",
       " '/c/en/be',\n",
       " '/c/en/minister',\n",
       " '/c/en/binna',\n",
       " '/c/en/pluperfect_tense/n',\n",
       " '/c/en/wast',\n",
       " '/c/en/were',\n",
       " '/c/en/wert',\n",
       " '/c/en/was/v',\n",
       " '/c/en/bee',\n",
       " '/c/en/complete',\n",
       " '/c/en/bell',\n",
       " '/c/en/went',\n",
       " '/c/en/is',\n",
       " '/c/en/statue',\n",
       " '/c/en/native',\n",
       " '/c/en/beeth',\n",
       " '/c/en/grass',\n",
       " '/c/en/present_perfect_continuous/n',\n",
       " '/c/en/cake',\n",
       " '/c/en/over',\n",
       " '/c/en/loot',\n",
       " '/c/en/always',\n",
       " '/c/en/am/v',\n",
       " '/c/en/fact',\n",
       " '/c/en/bread',\n",
       " '/c/en/floor',\n",
       " '/c/en/be/v',\n",
       " '/c/en/famous',\n",
       " '/c/en/occasional/a',\n",
       " '/c/en/frightful/a',\n",
       " '/c/en/transvector/n',\n",
       " '/c/en/trouble',\n",
       " '/c/en/torture/n',\n",
       " '/c/en/chilling/a',\n",
       " '/c/en/shadow',\n",
       " '/c/en/tool',\n",
       " '/c/en/queasy/a',\n",
       " '/c/en/casus_belli/n',\n",
       " '/c/en/dreadful/a',\n",
       " '/c/en/goshwow/a',\n",
       " '/c/en/self_perpetuating/a',\n",
       " '/c/en/causal/a',\n",
       " '/c/en/let_someone_down_gently/v',\n",
       " '/c/en/shameful/a',\n",
       " '/c/en/poison',\n",
       " '/c/en/pot/n/wikt/en_1',\n",
       " '/c/en/facient',\n",
       " '/c/en/drive',\n",
       " '/c/en/cut',\n",
       " '/c/en/deathly/a',\n",
       " '/c/en/terrific/a',\n",
       " '/c/en/doubtful/a',\n",
       " '/c/en/self_perpetuation/n',\n",
       " '/c/en/moose',\n",
       " '/c/en/mother_of_all',\n",
       " '/c/en/bear',\n",
       " '/c/en/calgary/n',\n",
       " '/c/en/major_diameter/n',\n",
       " '/c/en/montreal/n',\n",
       " '/c/en/sperm_whale/n',\n",
       " '/c/en/skin',\n",
       " '/c/en/monster_group/n',\n",
       " '/c/en/continent',\n",
       " '/c/en/capital',\n",
       " '/c/en/hamhung/n',\n",
       " '/c/en/trumpeter_swan/n',\n",
       " '/c/en/maximal/a',\n",
       " '/c/en/large/a',\n",
       " '/c/en/penghu/n',\n",
       " '/c/en/komodo_dragon/n',\n",
       " '/c/en/whale',\n",
       " '/c/en/main/a/wikt/en_1',\n",
       " '/c/en/sun',\n",
       " '/c/en/eris/n',\n",
       " '/c/en/1_as/n',\n",
       " '/c/en/2/a',\n",
       " '/c/en/one/n',\n",
       " '/c/en/har1',\n",
       " '/c/en/true/a',\n",
       " '/c/en/slang',\n",
       " '/c/en/this',\n",
       " '/c/en/arc',\n",
       " '/c/en/0/n',\n",
       " '/c/en/one',\n",
       " '/c/en/repunit/n',\n",
       " '/c/en/internet_slang',\n",
       " '/c/en/atomic_number_of_hydrogen',\n",
       " '/c/en/coprime/a',\n",
       " '/c/en/abstaction',\n",
       " '/c/en/draft',\n",
       " '/c/en/sarcastic',\n",
       " '/c/en/1s/n',\n",
       " '/c/en/pronounced_one',\n",
       " '/c/en/relatively_prime/a',\n",
       " '/c/en/leet',\n",
       " '/c/en/binary/n',\n",
       " '/c/en/like',\n",
       " '/c/en/equal_1',\n",
       " '/c/en/1st',\n",
       " '/c/en/logic_gate/n',\n",
       " '/c/en/binary_digit/n',\n",
       " '/c/en/l1',\n",
       " '/c/en/set_containing_one_element',\n",
       " '/c/en/binary_number/n',\n",
       " '/c/en/0',\n",
       " '/c/en/higher_number_than_0',\n",
       " '/c/en/boolean/a',\n",
       " '/c/en/ps_1/n',\n",
       " '/c/en/internet',\n",
       " '/c/en/boolean/n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
