{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL\n",
    "import os\n",
    "import json\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# 3rd Party\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from transformers import  BertTokenizerFast\n",
    "# Local\n",
    "from gatbert.data import parse_graph_tsv, Sample\n",
    "from gatbert.graph_sample import GraphSample\n",
    "from gatbert.constants import NodeType, Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(parse_graph_tsv('scrap.tsv'))\n",
    "\n",
    "sample = GraphSample(\n",
    "    stance=Stance.FAVOR,\n",
    "    target=[\"Pakistan\", \"government\"],\n",
    "    context=\"We need to stop supporting governmentalists who harbor terrorists .\".split(),\n",
    "    kb=[\"/c/en/governmentalists\", \"/c/en/pakistan\"],\n",
    "    edges=[]\n",
    ")\n",
    "clean_kb_sample = [uri.split('/')[3] for uri in sample.kb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer(text=sample.target, text_pair=sample.context, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "tokenized_kb = tokenizer(text=clean_kb_sample, is_split_into_words=True, return_offsets_mapping=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  4501,  2231,   102,  2057,  2342,  2000,  2644,  4637, 10605,\n",
       "          5130,  2040,  6496, 15554,  1012,   102]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'pakistan',\n",
       " 'government',\n",
       " '[SEP]',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'supporting',\n",
       " 'governmental',\n",
       " '##ists',\n",
       " 'who',\n",
       " 'harbor',\n",
       " 'terrorists',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0,  8],\n",
       "         [ 0, 10],\n",
       "         [ 0,  0],\n",
       "         [ 0,  2],\n",
       "         [ 0,  4],\n",
       "         [ 0,  2],\n",
       "         [ 0,  4],\n",
       "         [ 0, 10],\n",
       "         [ 0, 12],\n",
       "         [12, 16],\n",
       "         [ 0,  3],\n",
       "         [ 0,  6],\n",
       "         [ 0, 10],\n",
       "         [ 0,  1],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'governmental', '##ists', 'pakistan', '[SEP]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_kb['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0, 12],\n",
       "         [12, 16],\n",
       "         [ 0,  8],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_kb['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# (node_index, subword_index)\n",
    "pool_inds = []\n",
    "expand_list = defaultdict(list)\n",
    "\n",
    "new_nodes_index = -1\n",
    "orig_nodes_index = -1\n",
    "\n",
    "# For token subwords, we will split a token's nodes into subwords\n",
    "token_offset_mapping = tokenized_text['offset_mapping'].squeeze()\n",
    "# Handle splitting of token nodes into subword nodes\n",
    "for (subword_index, (start, end)) in enumerate(token_offset_mapping):\n",
    "    new_nodes_index += 1\n",
    "    if start != end: # Real character, not a special character\n",
    "        if start == 0: # Start of a token\n",
    "            orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds.append((new_nodes_index, subword_index))\n",
    "\n",
    "# For KB subwords, we plan to pool each into one combined node\n",
    "# Get this working next\n",
    "kb_offset_mapping = tokenized_kb['offset_mapping'].squeeze()\n",
    "for (subword_index, (start, end)) in enumerate(kb_offset_mapping, start=subword_index + 1):\n",
    "    if start == end:\n",
    "        # Special character; skip over\n",
    "        new_nodes_index += 1\n",
    "    elif start == 0:\n",
    "        new_nodes_index += 1\n",
    "        orig_nodes_index += 1\n",
    "        expand_list[orig_nodes_index].append(new_nodes_index)\n",
    "    pool_inds.append((new_nodes_index, subword_index))\n",
    "num_new_nodes = new_nodes_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
