{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL\n",
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# 3rd Party\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(0)\n",
    "from transformers import BertModel, AutoModel, BertTokenizerFast, AutoTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers.pre_tokenizers import BertPreTokenizer, PreTokenizer\n",
    "# Local\n",
    "from gatbert.constants import DEFAULT_MODEL, Stance\n",
    "from gatbert.datasets import MapDataset, make_encoder, make_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "@dataclasses.dataclass\n",
    "class Sample:\n",
    "    context: str\n",
    "    target: str\n",
    "    stance: Stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "# import numpy as np\n",
    "# def make_encoder(tokenizer: PreTrainedTokenizerFast, pretokenizer: Optional[PreTokenizer] = None):\n",
    "#     relation_types = [\n",
    "#         0, # Token-to-Token\n",
    "#         1, # Token-to-CN\n",
    "#         2, # CN-to-Token\n",
    "#         3, # CN-to-CN\n",
    "#     ]\n",
    "\n",
    "#     # Node indices start at 1 so we can use 0 for a \"padding node\"\n",
    "#     node_universe = np.arange(1, 1001)\n",
    "\n",
    "#     gen = torch.Generator().manual_seed(0)\n",
    "\n",
    "#     def encode_sample(sample: Sample):\n",
    "#         context = sample.context\n",
    "#         target = sample.target\n",
    "#         stance = sample.stance.value\n",
    "\n",
    "#         if pretokenizer:\n",
    "#             pre_context = [pair[0] for pair in pretokenizer.pre_tokenize_str(context)]\n",
    "#             pre_target = [pair[0] for pair in pretokenizer.pre_tokenize_str(target)]\n",
    "#             result = tokenizer(text=pre_target, text_pair=pre_context, is_split_into_words=True, return_tensors='pt')\n",
    "#         else:\n",
    "#             result = tokenizer(text=target, text_pair=context)\n",
    "\n",
    "#         result = {k: torch.squeeze(v) for (k, v) in result.items()}\n",
    "#         n_text_nodes = len(result['input_ids'])\n",
    "\n",
    "#         edge_ids = []\n",
    "#         for head in range(n_text_nodes):\n",
    "#             edge_ids.append( (head, head, 0) )\n",
    "#             for tail in range(head + 1, n_text_nodes):\n",
    "#                 edge_ids.append( (head, tail, 0) )\n",
    "#                 edge_ids.append( (tail, head, 0) )\n",
    "\n",
    "#         # Right now can only make fake KB nodes\n",
    "#         num_kb_nodes = torch.randint(3, 10, size=(), generator=gen)\n",
    "#         chosen_nodes = torch.tensor(np.random.choice(node_universe, size=int(num_kb_nodes), replace=False))\n",
    "#         for (token_id, cn_id) in zip(*torch.where(torch.randn(n_text_nodes, num_kb_nodes, generator=gen) < .1)):\n",
    "#             edge_ids.append( (token_id, cn_id + n_text_nodes, 1) )\n",
    "#         for (token_id, cn_id) in zip(*torch.where(torch.randn(n_text_nodes, num_kb_nodes, generator=gen) < .1)):\n",
    "#             edge_ids.append( (cn_id + n_text_nodes, token_id, 2) )\n",
    "#         for (cn_id, cn_id_b) in zip(*torch.where(torch.randn(num_kb_nodes, num_kb_nodes, generator=gen) < .1)):\n",
    "#             edge_ids.append( (cn_id + n_text_nodes, cn_id_b + n_text_nodes, 3) )\n",
    "#         result['kb_ids'] = chosen_nodes\n",
    "\n",
    "#         edge_ids.sort()\n",
    "#         sparse_ids = torch.tensor(edge_ids).transpose(1, 0)\n",
    "#         result['edges'] = sparse_ids\n",
    "#         result['stance'] = torch.tensor(stance)\n",
    "\n",
    "#         return result\n",
    "\n",
    "#     return encode_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=True)\n",
    "encoder = make_encoder(tokenizer, BertPreTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_samples = [\n",
    "    Sample(\n",
    "        context=\"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\",\n",
    "        target=\"Independence from Britain\",\n",
    "        stance=Stance.FAVOR\n",
    "    ),\n",
    "    Sample(\n",
    "        context=\"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\",\n",
    "        target=\"Social Security\",\n",
    "        stance=Stance.NONE\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_collate_fn(tokenizer):\n",
    "#     def collate_fn(samples: Dict[str, Any]):\n",
    "#         batched = {}\n",
    "#         token_padding = tokenizer.pad_token_id\n",
    "#         type_padding = tokenizer.pad_token_type_id\n",
    "#         batched['input_ids'] = torch.nn.utils.rnn.pad_sequence([s['input_ids'] for s in samples], batch_first=True, padding_value=token_padding)\n",
    "#         batched['token_type_ids'] = torch.nn.utils.rnn.pad_sequence([s['token_type_ids'] for s in samples], batch_first=True, padding_value=type_padding)\n",
    "#         batched['kb_ids'] = torch.nn.utils.rnn.pad_sequence([s['kb_ids'] for s in samples], batch_first=True, padding_value=0)\n",
    "\n",
    "#         batched['attention_mask'] = batched['input_ids'] != token_padding\n",
    "#         batched['stance'] = torch.stack([s['stance'] for s in samples], dim=0)\n",
    "\n",
    "#         batch_edges = []\n",
    "#         for (i, sample_edges) in enumerate(map(lambda s: s['edges'], samples)):\n",
    "#             batch_edges.append(torch.concatenate([\n",
    "#                 torch.full(size=(1, sample_edges.shape[1]), fill_value=i),\n",
    "#                 sample_edges\n",
    "#             ]))\n",
    "#         batched['edges'] = torch.concatenate(batch_edges, dim=-1)\n",
    "#         return batched\n",
    "#     return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MapDataset([encoder(s) for s in fake_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(ds, batch_size=2, shuffle=False, collate_fn=make_collate_fn(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in loader:\n",
    "    print(d['kb_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 123\n",
    "attention_units = 53\n",
    "out_features = 264\n",
    "n_heads = 6\n",
    "n_relations = 7\n",
    "n_bases = 3\n",
    "max_nodes = 10\n",
    "batch_size = 5\n",
    "gen = torch.Generator().manual_seed(1)\n",
    "random_features = 5 * (torch.randn(batch_size, max_nodes, in_features, generator=gen) - .5)\n",
    "random_features.shape\n",
    "random_adj = torch.randint(0, 2, size=[batch_size, max_nodes, max_nodes, n_relations], generator=gen).to_sparse()\n",
    "random_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gatbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
